# HDFS 架构

`HDFS` 即 `Hadoop分布式文件系统`，它是`Hadoop`生态系统的核心组成部分。与传统的文件系统醉倒的区别是，它提供了高容错、高吞吐、可以处理大数据量的数据功能。

## 1. 设计目标

1. `高容错性`: 硬件故障是正常现象而不是意外，`HDFS`系统可能由成千上万的服务器组成，每台机器都存储着部分的系统数据，所以，当部分服务器发生故障时，需要能够快速的检测问题，并自动修复，成为`HDFS`必不可少的功能；
2. `流式数据访问`: 运行在`HDFS`上的应用程序通常需要对数据进行流式访问，但与那些运行在传统文件系统上的应用程序不同，`HDFS`上的应用程序对数据做的是批处理，为了提高`HDFS`的吞吐量，传统文件系统的部分特性会被舍弃；
3. `大数据集`: `HDFS`上的应用程序通常都是大数据集的，通常的文件大小在`GB`到`TB`。所以，`HDFS`被设计为支持大文件存储，还应该有大的聚集带宽；
4. `简单一致性`: `HDFS`支持一次写入多次读取的应用。文件一旦被创建、写入、关闭后，除了追加和截断，不能再更改，不能在任意节点进行更改。该思想简化的`HDFS`对数据一致性的实现要求，也提供了它的数据吞吐量；
5. `迁移计算比迁移数据便宜`: 一个计算行为当离它所要的数据越近的时候，它的执行效率更高，对于大数据集的计算，尤其如此。通过迁移计算的方式，可以减少网络拥塞，也可以提高数据的吞吐量。`HDFS`提供了迁移计算的接口，从而让计算离它的数据更近；
6. `跨平台`: `HDFS`支持不同的硬件和软件系统平台，所以可以将其扩展到一个超级大的应用集群中；

## 2. 架构

![](hdfsarchitecture.png)

`HDFS`系统是`master/slave`架构，它由一个`NameNode`和多个`DataNode`组成。其中`NameNode`是`master`，主要负责管理文件系统命名空间和访问权限。`DataNode`可以有很多个， 通常是一台机器一个实例。`NameNode`和`DataNode`均由Java实现，所以任何可以运行Java的机器的都可以部署，通常`NameNode`单独部署到一台机器上。下面详细列出的二者的各自指责。

- `NameNode`:
  - 管理`HDFS`的命名空间和文件访问权限；
  - 存储文件与文件快的元数据信息；
  - 负责响应客户端查询文件信息请求的响应；
  - 管理文件；
- `DataNode`:
  - 实际存储数据，文件会被分割成按块存储；
  - 负责响应客户端读写文件内容请求的响应；


###  3. 文件系统命名空间

`HDFS`支持传统的文件系统结构，用户和应用程序都可以创建目录和将文件存入该目录。`HDFS`的文件系统命名空间与现有的其他文件系统蕾西，支持创建、删除文件，也支持移动文件到目录和冲命名文件。`HDFS`支持用户配额和访问权限设置。但暂未支持软链接和硬链接。

虽然`HDFS`的文件命名约定跟现有的文件系统一样，但对于像`.reserved`或`.snapshot`之类的，暂时保留。
`NameNode`保存文件的愿数据信息，任何对文件系统的修改或文件属性的修改都会被`NameNode`记录和存储。

### 4. 数据副本

`HDFS` 是为了可靠地在大集群中存储大数据文件而设计的。它将文件分成一系列的块，对这些块再存储副本来保证容错性。每个文件中，块的大小和副本因子都是可以设置的。

一个文件的所有块的大小是一样的，除了该文件的最后一个块。副本因子可以再创建文件的时候指定，也可以在后面进行修改。`HDFS` 中任何时候只支持单用户和一次写入，包括对文件进行追加或截断。

`NameNode` 负责所有块的副本做出决策。它会定期从集群中的每个 `DataNode` 中接收 `HeartBeat` 和`Block Report` ，`HeartBeat`用于保证 `DataNode` 运行正常，`Block Report` 用于上报`DatabaNode`中的块信息。

<img src="hdfsdatanodes.png" style="zoom:75%;" />

#### 4.1 副本放置策略

副本的放置对 `HDFS` 的可靠性和性能是非常重要的，这个需要我们不断的测试和总结经验。机架感知策略的目标是为了提高数据的可靠性、可用性以及提高网络带宽的利用率。

对于大型的 `HDFS` 机器来说，它的实例经常分不在不同的机架上，在不同机架的两天机器要通信必须通过交换机。通常情况下，在相同机架上的两台机器的的带宽通常比不同机架上的两台机器的带宽大。

`NameNode` 是决定每个 `DataNode` 所属的机架。一种简单的副本放置策略是，将每个副本分布在不同机架的某台机器上，这可以保证在某个机架完全不可用时，数据不会丢失，也可以在提高读数据时，充分利用多个机架的带宽。这种策略在有组件发生故障时，可以很好的平衡各个机架的负载。然而，这种策略也会显著的提高写消耗，因为需要在不同的机架上拷贝数据。

更常用的策略是，当副本因子为3时，`HDFS` 将第一个副本放置在应用程序所属的机器上(如果应用程序在 `DataNode` 上运行，否则就选择应用程序所属机架的随机`DataNode`中)；第2，3个副本放置在其他机架的随机两台 `DataNode`中。该策略减少了机架间的写流量，从而提高了写操作的性能。现实情况中，机架发生故障比机器发生故障的概率小很多，所以，这种策略也不会影响数据的可靠性和可用性。

在 `Storage Types` 和 `Storage Policies` 加入 `HDFS` 后，`NameNode` 通过机架感知策略来放置副本。当 `NameNode` 在机架感知的第一层机器中，如果候选机器满足文件的 `Stroage Type` 和 `Storage Policies` 要求，则在该层机器中选取基准机器；否则，它会寻找第二层满足要求的机器。

## 5. 文件系统元数据的持久化

`HDFS` 的元数据存储在 NameNode 中，`NameNode` 使用事务日志 `Editlog` 来持久化存储文件系统元数据的修改。完整的 `HDFS` 命名空间信息和块与文件的映射信息，则保存在 `FsImage` 文件中。这两个文件均由 `NameNode` 本地文件系统存储。

`NameNode` 中会保存一份完整的命名空间信息、块和文件的映射信息到内存中。当 NameNode 启动的时，或者检查点被触发时，就会从磁盘中读取 `FsImage`和 `Ediglog` 的信息，加载到内存中。运行过程中，内存中的信息也会定时刷新到磁盘的 `FsImage` 文件中，然后就可以截断过时的 `Editlog` 里的数据，这个过程就叫做检查点`(Checkpoint)`。通过检查点，我们就可以通过将元数据信息不断做快照，并保存到 `FsImage` 中，来保证 `HDFS` 文件系统与元数据信息的一致性。

虽然读取 `FsImage` 文件的效率很高，但不断地对 `FsImage` 执行追加操作效率却不高，所以引入 `Ediglog`来记录命名空间的修改操作。当 `Checkpoint`执行时，`Editlog` 的修改会应用到 `FsImage`中。检查点的触发可以通过配置两个参数来实现：

* `dfs.namenode.checkpoint.period`: 检查点的触发时间间隔
* `dfs.namenode.checkpoint.txns`: 检查点触发的事务数

## 6. 可靠性保证

### 6.1 数据盘损坏

* `心跳`： 每个 `DataNode` 都会定时的发送 `HeartBeat` 到 `NameNode`, 网络分区时会导致部分 `DataNode` 无法连接 `NameNode`，`NameNode` 可以通过 `HeartBeat` 的接收情况来检测出这些节点，对一段时间未接收到 `HeartBeat` 的 `DataNode`，`NameNode` 会将它标记为 `dead`,并不会再转发任何 `IO` 请求到这些 `DataNode`；
* `重造副本`：对于被标记为 `dead` 的机器，其上面的副本数据都将不可再访问，这回导致集群中的部分数据块的副本因子小于文件的最小副本数，`NameNode` 会经常检查这些低于定义的最小副本数的数据块，并重新寻找节点对这些数据块新造副本。`重造副本` 操作可以被多种情况出发：节点宕机、副本损坏、磁盘损坏、增大副本因子等；但副本损坏或磁盘损坏需要再读过程中发现无法读取，再反馈给 `NameNode`来发现；

### 6.2 数据再均衡

`HDFS`的系统架构支持数据的再均衡。当某个 `DataNode`的空闲空间低于某个阈值时，会触发数据从该 `DataNode` 转移到其他的 `DataNode`; 当突然出现大量请求同一个文件时，也可以触发自动提高该文件数据块的副本数，来分散集群的负载。然而都还没实现；

### 6.3 数据完整性

当我们请求 `HDFS`中的数据时，发生数据丢失或损坏的可能性是有的，引起这个问题的通常原因有存储设备损坏、网络问题或者是软件的bug，`HDFS` 的客户端实现了校验和检测方案。每当客户端上传文件到 `HDFS` 时，会同时计算文件的校验和并将校验和文件上传到相同命名空间的隐藏文件中；当从 `HDFS` 中读取文件时，也会同时读取对应的校验和文件，并计算校验和与之比较。如果比对不一致，则从其他节点获取对应数据块的副本；

### 6.4 元数据

元数据对 `HDFS`系统是非常重要的数据，元数据的损坏会导致 `NameNode`无法启动。对此，可以有如下几种方法，来保证元数据的可靠性

* 对 `FsImage` 和 `Editlog` 文件定时备份；
* 高可用 `NameNode` 集群，并使用共享的 `NFS` 上的 `FsImage`、 `Editlog`;
* `Journal Log`

## 7. 数据组织

### 7.1 数据块

`HDFS`是为处理大数据文件而设计的，对应的应用程序也是大数据集的。一般情况下，这些应用程序仅一次写入数据，但会多次读这些数据，而且要求读数据的效率能满足流式处理速度，`HDFS`支持一次写入多次读取的场景。`HDFS`上的标准的数据块大小的128M，所以 `HDFS`上的文件被切分为128M，每个块一般会分配到不同的 `DataNode`上。

### 7.2 副本复制流程

假设一个客户段写一个副本因子为3的文件到 `HDFS`上，`NameNode` 会得到有副本节点选择算法返回的 `DataNode`列表，这些 `DataNode`就是用来存该文件的文件块的节点。然后客户端会先写第一个 `DataNode`，该 `DataNode`会开始接收数据块，同时，也会将数据块发送到第二个 `DataNode`;同理，第二个 `DataNode`接收数据块时会将数据块发送给第三个 `DataNode`，写完后递归返回响应给上一级，知道客户端收到请求。


# HDFS 读写操作

## 3. 读流程





# HDFS 管理

## 



## 

参考链接：
[HDFS Architecture](https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication])